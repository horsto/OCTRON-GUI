{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Napari tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "cur_path = Path(os.getcwd()).parent\n",
    "sam2_path = cur_path / 'sam2_octron'\n",
    "sys.path.append(cur_path.as_posix())\n",
    "from matplotlib import pyplot as plt\n",
    "import cmasher as cmr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='white')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "from napari.qt.threading import thread_worker\n",
    "from napari.utils import DirectLabelColormap\n",
    "from napari.utils.notifications import show_info, show_warning\n",
    "import warnings\n",
    "warnings.simplefilter(action='always', category=FutureWarning)\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "#### Importing additional stuff \n",
    "from skimage import measure\n",
    "from skimage.draw import polygon2mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the SAM2 model like you do in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2_octron.helpers.sam2_checks import check_model_availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_yaml_path = sam2_path / 'models.yaml'\n",
    "models_dict = check_model_availability(SAM2p1_BASE_URL='',\n",
    "                         models_yaml_path=models_yaml_path,\n",
    "                         force_download=False,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "from octron.sam2_octron.helpers.build_sam2_octron import build_sam2_octron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful . these path descriptors differ slightly between notebook and \n",
    "# plugin version\n",
    "model = models_dict['sam2_large']\n",
    "\n",
    "config_path = Path(model['config_path'])\n",
    "checkpoint_path = sam2_path / Path(f\"{model['checkpoint_path']}\")\n",
    "predictor, device  = build_sam2_octron(config_file=config_path.as_posix(), \n",
    "                                       ckpt_path=checkpoint_path.as_posix(), \n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Zarr array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octron.sam2_octron.helpers.zarr_archives import create_image_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From napari, after loading video file, extract info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.dims.set_point(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_layers = []\n",
    "\n",
    "for l in viewer.layers:\n",
    "    if l._basename() == 'Image':\n",
    "        l.editable = False\n",
    "        layer_dict = l.source.dict()\n",
    "        layer_dict['id'] = l.unique_id\n",
    "        layer_dict['data'] = l.data\n",
    "        layer_dict['shortened_name'] = '...' + l.name[-10:]\n",
    "        layer_dict['num_frames'] = l.data.shape[0]\n",
    "        layer_dict['height'] = l.data.shape[1]\n",
    "        layer_dict['width'] = l.data.shape[2]\n",
    "        layer_dict['nchannels'] = l.data.shape[3]\n",
    "        image_layers.append(layer_dict)\n",
    "\n",
    "assert len(image_layers) == 1, 'Only one image layer is allowed. Please remove the rest.'\n",
    "if image_layers:\n",
    "    for l in image_layers:\n",
    "        short_name = l['shortened_name']\n",
    "        show_info(f'ðŸ–¥ï¸ Found image data layer \"{short_name}\" with {l[\"num_frames\"]} frames, {l[\"height\"]}x{l[\"width\"]} pixels, and {l[\"nchannels\"]} channels.')\n",
    "        \n",
    "        \n",
    "num_frames = image_layers[0]['num_frames']\n",
    "video_height = image_layers[0]['height']\n",
    "video_width = image_layers[0]['width']\n",
    "video_nchannels = image_layers[0]['nchannels']\n",
    "video_data = image_layers[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp output dir \n",
    "sample_dir = cur_path / 'sample_data'\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "sample_data_zarr = sample_dir / 'sample_data.zip'\n",
    "\n",
    "image_zarr = create_image_zarr(sample_data_zarr,\n",
    "                               num_frames=num_frames,\n",
    "                               image_height=predictor.image_size,\n",
    "                               chunk_size=chunk_size,\n",
    "                               )\n",
    "image_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.init_state(video_data=video_data, zarr_store=image_zarr)\n",
    "predictor.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to load previous checkpoint does not work \n",
    "# Since the model still expects at least one input\n",
    "\n",
    "# import torch \n",
    "# state_path = '/Users/horst/Documents/python/OCTRON/octron/sample_data/model_output.pth'\n",
    "# checkpoint = torch.load(state_path, weights_only=True)\n",
    "# predictor.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_pred(frame_idx,\n",
    "                 obj_id, \n",
    "                 label,\n",
    "                 point=None,\n",
    "                 mask=None\n",
    "                 ):\n",
    "    assert label in [0,1], f'label must be 0 or 1, got {label}'\n",
    "    assert point is not None or mask is not None\n",
    "    if mask is not None:\n",
    "        assert len(mask.shape) == 2\n",
    "        \n",
    "        print('Running mask prediction')\n",
    "        frame_idx, obj_ids, video_res_masks = predictor.add_new_mask(\n",
    "                                                    frame_idx=frame_idx,\n",
    "                                                    obj_id=obj_id,\n",
    "                                                    mask=np.array(mask, dtype=bool)\n",
    "                                                    )\n",
    "        mask = (video_res_masks[obj_id] > 0).cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "        \n",
    "    if point is not None:\n",
    "        assert len(point) == 2\n",
    "        # Run point prediction\n",
    "        print('Running point prediction')\n",
    "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                                                    frame_idx=frame_idx,\n",
    "                                                    obj_id=obj_id,\n",
    "                                                    points=np.array([point],dtype=np.float32),\n",
    "                                                    labels=np.array([label], np.int32)\n",
    "                                                    )\n",
    "        \n",
    "        # Add the mask image as a new labels layer\n",
    "        mask = (out_mask_logits[obj_id] > 0).cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "    current_label = obj_id+1\n",
    "    if len(np.unique(mask))>1:\n",
    "        mask[mask==np.unique(mask)[1]] = current_label \n",
    "    mask = mask.squeeze()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up thread worker to deal with prefetching batches of images\n",
    "@thread_worker\n",
    "def thread_prefetch_images(batch_size):\n",
    "    print('Running prefetcher')\n",
    "    global viewer\n",
    "    current_indices = viewer.dims.current_step\n",
    "    _ = predictor.images[slice(current_indices[0],current_indices[0]+batch_size)]\n",
    "prefetcher_worker = thread_prefetch_images(chunk_size)   \n",
    "prefetcher_worker.setAutoDelete(False)\n",
    "prefetcher_worker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create a continous colormap and cmap_range for each label \n",
    "# --> this way we get sub colormaps and for the same label name can cycle through these \n",
    "def create_label_colors(cmap='cmr.tropical', label_n=4, obj_n=5):\n",
    "    '''\n",
    "    Create color map dictionary for labels \n",
    "    label(int) -> color list -> color(4D)\n",
    "    \n",
    "    For each label (n=label_cat_n) create a sub colormap with color_cat_n colors.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    label_cat_rel = np.linspace(0,1,label_n+1) # This must be the ugliest written fctn in the world\n",
    "\n",
    "    label_colors = {}\n",
    "    for cat in range(len(label_cat_rel)-1):\n",
    "        rel_range = label_cat_rel[cat:cat+2]\n",
    "        colors = np.array(cmr.take_cmap_colors(cmap, \n",
    "                                              obj_n, \n",
    "                                              cmap_range=(rel_range[0], rel_range[1]), \n",
    "                                              return_fmt='int'\n",
    "                                              ) \n",
    "                        ) / 255.0\n",
    "        colors4d = np.hstack([colors, np.ones((len(colors), 1))])\n",
    "        label_colors[cat] = {i+1: color for i, color in enumerate(colors4d)} # Keys start at 1 !\n",
    "        label_colors[cat][None] = np.array([0,0,0,0]).astype(np.float32)\n",
    "    return label_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 1 # Only plays a role here ... not in predictor  \n",
    "obj_id = 0\n",
    "colors = create_label_colors(cmap='cmr.tropical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the mask and annotation layers \n",
    "# Keep them empty at start \n",
    "mask_layer_dummy = np.zeros((num_frames, video_height, video_width), dtype=np.uint8)\n",
    "mask_layer_dummy.shape\n",
    "\n",
    "\n",
    "# Select colormap for labels layer based on category (label) and current object ID \n",
    "current_color_labelmap = DirectLabelColormap(color_dict=colors[label_id], \n",
    "                                             use_selection=True, \n",
    "                                             selection=obj_id+1,\n",
    "                                             )\n",
    "labels_layer = viewer.add_labels(\n",
    "    mask_layer_dummy, \n",
    "    name='Mask',  # Name of the layer\n",
    "    opacity=0.4,  # Optional: opacity of the labels\n",
    "    blending='additive',  # Optional: blending mode\n",
    "    colormap=current_color_labelmap, \n",
    ")\n",
    "\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[labels_layer]\n",
    "buttons_to_hide = ['erase_button',\n",
    "                   'fill_button',\n",
    "                   'paint_button',\n",
    "                   'pick_button',\n",
    "                   'polygon_button',\n",
    "                   'transform_button',\n",
    "                   ]\n",
    "for btn in buttons_to_hide:\n",
    "    getattr(qctrl, btn).setEnabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the shapes layer to the viewer\n",
    "shapes_layer = viewer.add_shapes(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Shape annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 edge_width=0,\n",
    "                                 face_color=colors[label_id][obj_id+1],\n",
    "                                 opacity=.5,\n",
    "                                 )\n",
    "\n",
    "# Store the initial length of the shapes data\n",
    "previous_length_shapes = len(shapes_layer.data)\n",
    "\n",
    "# Function to convert polygon data to mask\n",
    "def polygons_to_mask(polygon, shape):\n",
    "    mask = np.zeros(shape, dtype=np.uint8)\n",
    "    mask = polygon2mask(mask.shape, polygon)\n",
    "    mask = mask.astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "def on_shapes_added(event):\n",
    "    global shapes_layer\n",
    "    global labels_layer\n",
    "    global previous_length_shapes\n",
    "    global height, width\n",
    "    current_length = len(shapes_layer.data)\n",
    "    if current_length > previous_length_shapes:\n",
    "        previous_length_shapes = current_length \n",
    "\n",
    "        # Execute prediction \n",
    "        newest_shape_data =  shapes_layer.data[-1]    \n",
    "        frame_idx = int(newest_shape_data[:,0][0])\n",
    "        print(frame_idx)\n",
    "        input_mask = polygons_to_mask(newest_shape_data[:,1:], (video_height, video_width))\n",
    "        label = 1\n",
    "        mask = run_new_pred(frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            mask=input_mask,\n",
    "                            )\n",
    "\n",
    "        labels_layer.data[frame_idx] = mask\n",
    "        labels_layer.refresh()\n",
    "        \n",
    "        # Delete recently added shapes data\n",
    "        shapes_layer.data = [] \n",
    "        shapes_layer.refresh()\n",
    "        # Prefetch batch of images\n",
    "        # This is done here since adding it as direct mouse interaction \n",
    "        # slows down the first prediction\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "        \n",
    "    return\n",
    "\n",
    "# Store the initial length of the points data\n",
    "# previous_length_points = len(shapes_layer.data)\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[shapes_layer]\n",
    "buttons_to_hide = ['transform_button', \n",
    "                   'delete_button', \n",
    "                   'select_button', \n",
    "                   'direct_button',\n",
    "                   'ellipse_button',\n",
    "                   'line_button',\n",
    "                   'move_back_button',\n",
    "                   'move_front_button',\n",
    "                   'path_button',\n",
    "                   'polygon_button',\n",
    "                   'polyline_button',\n",
    "                   'rectangle_button',\n",
    "                   'vertex_insert_button',\n",
    "                   'vertex_remove_button',\n",
    "\n",
    "                   ]\n",
    "for btn in buttons_to_hide:\n",
    "    getattr(qctrl, btn).setEnabled(False)\n",
    "\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = shapes_layer\n",
    "viewer.layers.selection.active.mode = 'pan_zoom'\n",
    "\n",
    "shapes_layer.events.data.connect(on_shapes_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_layer.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the points layer to the viewer\n",
    "points_layer = viewer.add_points(None, \n",
    "                                 ndim=3,\n",
    "                                 name='Annotations', \n",
    "                                 scale=(1,1),\n",
    "                                 size=40,\n",
    "                                 border_color='dimgrey',\n",
    "                                 border_width=.2,\n",
    "                                 opacity=.6,\n",
    "                                 )\n",
    "# Store the initial length of the points data\n",
    "previous_length_points = len(points_layer.data)\n",
    "\n",
    "\n",
    "left_right_click = 'left'\n",
    "def on_mouse_press(layer, event):\n",
    "    '''\n",
    "    Generic function to catch left and right mouse clicks\n",
    "    '''\n",
    "    global left_right_click\n",
    "    if event.type == 'mouse_press':\n",
    "        if event.button == 1:  # Left-click\n",
    "            left_right_click = 'left'\n",
    "        elif event.button == 2:  # Right-click\n",
    "            left_right_click = 'right'     \n",
    "    \n",
    "\n",
    "def on_points_added(event):\n",
    "    '''\n",
    "    Function to run when points are added to the points layer\n",
    "    '''\n",
    "    \n",
    "    global points_layer\n",
    "    global labels_layer\n",
    "    global left_right_click\n",
    "    global prefetcher_worker\n",
    "    global previous_length_points\n",
    "    \n",
    "    current_length = len(points_layer.data)\n",
    "    if current_length > previous_length_points:\n",
    "        previous_length_points = current_length \n",
    "\n",
    "        # Execute prediction \n",
    "        newest_point_data =  points_layer.data[-1]\n",
    "        if left_right_click == 'left':\n",
    "            label = 1\n",
    "            points_layer.face_color[-1] = [0.59607846, 0.98431373, 0.59607846, 1.]\n",
    "            points_layer.symbol[-1] = 'o'\n",
    "        elif left_right_click == 'right':\n",
    "            label = 0\n",
    "            points_layer.face_color[-1] = [1., 1., 1., 1.]\n",
    "            points_layer.symbol[-1] = 'x'\n",
    "        points_layer.refresh() \n",
    "        # Run prediction\n",
    "        frame_idx  = int(newest_point_data[0])\n",
    "        point_data = newest_point_data[1:][::-1]\n",
    "        mask = run_new_pred(frame_idx=frame_idx,\n",
    "                            obj_id=0,\n",
    "                            label=label,\n",
    "                            point=point_data,\n",
    "                            )\n",
    "        labels_layer.data[frame_idx,:,:] = mask\n",
    "        labels_layer.refresh()   \n",
    "        \n",
    "        # Prefetch batch of images\n",
    "        # This is done here since adding it as direct mouse interaction \n",
    "        # slows down the first prediction\n",
    "        if not prefetcher_worker.is_running:\n",
    "            prefetcher_worker.run()\n",
    "\n",
    "\n",
    "points_layer.mouse_drag_callbacks.append(on_mouse_press)\n",
    "points_layer.events.data.connect(on_points_added)\n",
    "\n",
    "# Hide the transform, delete, and select buttons\n",
    "qctrl = viewer.window.qt_viewer.controls.widgets[points_layer]\n",
    "buttons_to_hide = ['transform_button', \n",
    "                   'delete_button', \n",
    "                   'select_button', \n",
    "]\n",
    "for btn in buttons_to_hide:\n",
    "    getattr(qctrl, btn).setEnabled(False)\n",
    "                   \n",
    "\n",
    "# Select the current, add tool for the points layer\n",
    "viewer.layers.selection.active = points_layer\n",
    "viewer.layers.selection.active.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = 0\n",
    "\n",
    "@thread_worker\n",
    "def thread_predict(frame_idx, max_imgs):\n",
    "    global labels_layer\n",
    "\n",
    "    video_segments = {} \n",
    "    start_time = time.time()\n",
    "    # Prefetch images if they are not cached yet \n",
    "    _ = predictor.images[slice(frame_idx,frame_idx+max_imgs)]\n",
    "    \n",
    "    # Loop over frames and run prediction (single frame!)\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(start_frame_idx=frame_idx, \n",
    "                                                                                    max_frame_num_to_track=max_imgs):\n",
    "        \n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            \n",
    "            torch_mask = out_mask_logits[i] > 0.0\n",
    "            # torch_mask = torch_mask.float()\n",
    "            # torch_mask = torch_mask[torch_newaxis,:,:,:]  \n",
    "          \n",
    "            #Perform morphological closing \n",
    "            # torch_mask = kornia_closing(torch_mask, kernel)\n",
    "            out_mask = torch_mask.cpu().numpy()\n",
    "\n",
    "            video_segments[out_frame_idx] = {out_obj_id: out_mask}\n",
    "            if not out_obj_id in predictor.inference_state['centroids']:\n",
    "                predictor.inference_state['centroids'][out_obj_id] = {}\n",
    "            if not out_obj_id in predictor.inference_state['areas']:\n",
    "                predictor.inference_state['areas'][out_obj_id] = {}\n",
    "                \n",
    "        # PICK ONE OBJ (OBJ_ID = 0 or whatever)\n",
    "        \n",
    "        #  Add the mask image as a new labels layer\n",
    "        mask = video_segments[out_frame_idx][obj_id] # THIS NEEDS TO BE MADE LAYER SPECIFIC \n",
    "        current_label = obj_id+1\n",
    "        if len(np.unique(mask))>1:\n",
    "            mask[mask==np.unique(mask)[1]] = current_label \n",
    "\n",
    "        mask = mask.squeeze()\n",
    "        props = measure.regionprops(mask.astype(int))[0]\n",
    "        predictor.inference_state['centroids'][obj_id][out_frame_idx] = props.centroid\n",
    "        predictor.inference_state['areas'][obj_id][out_frame_idx] = props.area\n",
    "        labels_layer.data[out_frame_idx,:,:] = mask\n",
    "        viewer.dims.set_point(0,out_frame_idx)\n",
    "        labels_layer.refresh()\n",
    "    end_time = time.time()\n",
    "    #print(f'start idx {frame_idx} | {max_imgs} frames in {end_time-start_time} s')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.perform_morphological_operations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor as torch_tensor\n",
    "from skimage.morphology import disk\n",
    "\n",
    "predictor.perform_morphological_operations = True\n",
    "\n",
    "disk_size=10\n",
    "compute_device=device\n",
    "predictor.closing_kernel = torch_tensor(disk(disk_size).tolist()).to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current chunk size: {chunk_size}')\n",
    "worker = thread_predict(frame_idx=viewer.dims.current_step[0], max_imgs=chunk_size) \n",
    "#worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "worker.start()  # start the thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tried to save the checkpoint, \n",
    "# # but this does not work. \n",
    "# # the check point model_state does not contain enough info \n",
    "# import torch\n",
    "# model_output_path = sample_dir / 'model_output.pth'    \n",
    "# torch.save({\n",
    "#             'model_state_dict': predictor.state_dict(),\n",
    "#             }, model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the whole video as test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "for i in range(0,500,chunk_size):\n",
    "    \n",
    "    prediction_worker = thread_predict(frame_idx=i, max_imgs=chunk_size)  \n",
    "    prediction_worker.setAutoDelete(True)\n",
    "    #worker.returned.connect(viewer.add_image)  # connect callback functions\n",
    "    prediction_worker.start()  \n",
    "    print(f'Highest cached index {int(np.nanmax(predictor.images.cached_indices))}')\n",
    "    time.sleep(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style='white')\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "# mpl.rcParams.update({\"axes.grid\" : True, \"grid.color\": \"grey\", \"grid.alpha\": .1})\n",
    "# plt.rcParams['xtick.major.size'] = 10\n",
    "# plt.rcParams['xtick.major.width'] = 1\n",
    "# plt.rcParams['ytick.major.size'] = 10\n",
    "# plt.rcParams['ytick.major.width'] = 1\n",
    "# plt.rcParams['xtick.bottom'] = True\n",
    "# plt.rcParams['ytick.left'] = True\n",
    "# mpl.rcParams['savefig.pad_inches'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the centroids over time\n",
    "# centroids = list(predictor.inference_state['centroids'][0].values())\n",
    "# centroids = np.stack(centroids)\n",
    "# areas = np.array(list(predictor.inference_state['areas'][0].values())).astype(float)\n",
    "# figure = plt.figure(figsize=(10,10))\n",
    "# plt.imshow(viewer.layers[0].data[0], cmap='gray')\n",
    "# #plt.plot(centroids[:,1], centroids[:,0], '-', color='k', alpha=.6)   \n",
    "# plt.scatter(centroids[:,1], centroids[:,0], s=areas/50, marker='.', color='pink', alpha=.15, lw=0)   \n",
    "# sns.despine(left=True,bottom=True)\n",
    "# plt.title(f'Centroids over time (n={centroids.shape[0]} frames)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(predictor.inference_state['areas'][0].values()),'-', color='w', alpha=.6 )\n",
    "# plt.title('Area over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict_per_obj is huge \n",
    "# Structure\n",
    "# -> obj_id\n",
    "# --> cond_frame_outputs\n",
    "# --> non_cond_frame_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
